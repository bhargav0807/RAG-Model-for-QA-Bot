{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation (RAG) Model for Question Answering Bot\n",
    "\n",
    "#### Objective\n",
    "- The goal of this project is to create a QA bot that leverages a Retrieval-Augmented Generation (RAG) approach. This means combining the capabilities of both information retrieval (vector database - Pinecone) and generation (OpenAI GPT) to provide coherent and contextually relevant answers.\n",
    "\n",
    "#### Architecture and Flow\n",
    "1. **Data Loading and Preprocessing**: Load documents (I have used PDFs and can be modified easily for other data sources) and split them into chunks of text.\n",
    "2. **Embeddings**: Each chunk of text is converted into embeddings using OpenAI's embedding model (`text-embedding-ada-002`).\n",
    "3. **Vector Storage**: These embeddings are stored in Pinecone, a vector database optimized for similarity search.\n",
    "4. **Retrieval**: When a user submits a query, relevant document chunks are retrieved from Pinecone based on similarity.\n",
    "5. **Generative Model**: A generative model (GPT-3.5-turbo) uses the retrieved document chunks to generate a coherent response to the query.\n",
    "6. **Question Answering**: The system responds with an answer that is grounded in the retrieved information.\n",
    "\n",
    "#### Tools Used:\n",
    "- **LangChain**: For managing prompts, chains, and interaction between components.\n",
    "- **Pinecone**: As a vector database for efficient storage and retrieval of document embeddings.\n",
    "- **OpenAI**: For embedding and generating responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "\n",
    "from pinecone import Pinecone as PC\n",
    "from pinecone import ServerlessSpec\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Read the Documents\n",
    "\"\"\" Preprocessing techniques like removing extra white spaces, special characters, stop words. \n",
    "    lemmatisation, tokenisation, special data extraction tools for tables or images may increase the efficiency\n",
    "    for the specific use case.\n",
    "    \n",
    "    - For Text files, TextLoader() from Langchain Document loaders can be used.\n",
    "    - For CSV files, create_csv_agent from langchain agent tool kits can be used. \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def read_doc(doc):\n",
    "    loader = PyPDFLoader(doc)\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = read_doc(\"budget_speech.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chunking the data read from the pdf\n",
    "\n",
    "def chunk_data(docs, chunk_size=800, chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    doc=text_splitter.split_documents(docs)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = chunk_data(docs=doc)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuring the embedding model\n",
    "\n",
    "# embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting PineCone and Creating an Index\n",
    "\n",
    "import time\n",
    "\n",
    "pc = PC(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index_name = \"docs-rag\"\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if not pc.has_index(index_name):\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        ) \n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the chunks and Storing in Pinecone\n",
    "index=Pinecone.from_documents(chunks,embedding_model,index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring a Chat model\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "## Using Groq API\n",
    "# from langchain_groq import ChatGroq\n",
    "\n",
    "# chat=ChatGroq(groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "#              model_name=\"llama-3.1-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PromptTemplate for augmenting the prompt with retrieved knowledge\n",
    "\n",
    "def augment_prompt_with_template(query, k=5):\n",
    "    # Retrieve top 3 relevant chunks from the vectorstore\n",
    "    results = index.similarity_search(query, k=k)\n",
    "    \n",
    "    # Extract the text content from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    source = results[0].metadata\n",
    "    \n",
    "    # Define the prompt template with placeholders\n",
    "    template = \"\"\"\n",
    "                Your task is to create an answer to the user's query using the information\n",
    "                from the context provided. Follow these steps to generate the response:\n",
    "\n",
    "                Step 1: Analyze the user-provided query: {query}\n",
    "                Step 2: Review the relevant context provided: {contexts}\n",
    "                Step 3: Generate a concise, clear, and informative response based on the context, \n",
    "                        ensuring it addresses the query and maintains accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the LangChain PromptTemplate\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"contexts\", \"query\"],\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    # Create the final augmented prompt by filling the template\n",
    "    augmented_prompt = prompt_template.format(contexts=source_knowledge, query=query)\n",
    "    \n",
    "    return augmented_prompt, source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment prompt with vectorstore results using PromptTemplate\n",
    "def run_augmented_prompt(query):\n",
    "    \n",
    "    # Create initial message history\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant.\")\n",
    "    ]\n",
    "    \n",
    "    # Augment the user query with context from vectorstore using PromptTemplate\n",
    "    augmented_prompt_text, source = augment_prompt_with_template(query)\n",
    "    augmented_prompt = HumanMessage(content=augmented_prompt_text)\n",
    "    \n",
    "    # Append augmented prompt to messages\n",
    "    messages.append(augmented_prompt)\n",
    "    \n",
    "    # Interact with the chat model\n",
    "    res = chat(messages)\n",
    "    return res.content, source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RAG with queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anwer: The total budget for 2023-24 is estimated at ` 45 lakh crore, with total receipts other than borrowings at ` 27.2 lakh crore. The net tax receipts are estimated at ` 23.3 lakh crore. The fiscal deficit is projected to be 5.9 per cent of GDP, with net market borrowings from dated securities estimated at ` 11.8 lakh crore. \n",
      "Top Source: {'page': 28.0, 'source': 'budget_speech.pdf'}\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What is the total budget for 2023?\"\n",
    "answer, source = run_augmented_prompt(query1)\n",
    "print(f'Anwer: {answer} \\nTop Source: {source}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anwer: The income tax rate for the income range of 9-12 lakh is 15%. \n",
      "Top Source: {'page': 37.0, 'source': 'budget_speech.pdf'}\n"
     ]
    }
   ],
   "source": [
    "query2 = \"What is the income tax rate for 9-12 lakh?\"\n",
    "answer, source = run_augmented_prompt(query2)\n",
    "print(f'Anwer: {answer} \\nTop Source: {source}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anwer: The rate of excise duty on Cigarettes of tobacco substitutes is `600 per 1000 sticks to `690 per 1000 sticks, effective from 02.02.2023. \n",
      "Top Source: {'page': 57.0, 'source': 'budget_speech.pdf'}\n"
     ]
    }
   ],
   "source": [
    "query3 = \"What is the rate of excise duty on Cigarettes of tobacco substitutes?\"\n",
    "answer, source = run_augmented_prompt(query3)\n",
    "print(f'Anwer: {answer} \\nTop Source: {source}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If Advanced RAG is needed for specific use case, we can use Optimazation techniques like Self Query, Query Expansion, Hybrid Search, Re-ranking and filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thank You !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
